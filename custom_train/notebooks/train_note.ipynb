{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "162c2a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src path added: d:\\Code\\voice_clone\\local_test\\src\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "\n",
    "# Core ML/Data Libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import librosa\n",
    "import numpy as np\n",
    "import datasets\n",
    "from datasets import load_dataset, VerificationMode\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Hugging Face Trainer\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    set_seed,\n",
    "    TrainerCallback,\n",
    "    Trainer,\n",
    "    PretrainedConfig,\n",
    "    TrainingArguments as HfTrainingArguments,\n",
    "    # get_last_checkpoint\n",
    ")\n",
    "\n",
    "# Language and Text Processing\n",
    "from langdetect import detect\n",
    "import pykakasi\n",
    "from num2words import num2words # For our Spanish example\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), \"../../src\"))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "print(\"src path added:\", src_path)\n",
    "\n",
    "# Chatterbox Specific Imports\n",
    "from chatterbox.tts import ChatterboxTTS, punc_norm, REPO_ID\n",
    "from chatterbox.models.t3.t3 import T3, T3Cond\n",
    "from chatterbox.models.t3.modules.t3_config import T3Config\n",
    "from chatterbox.models.s3tokenizer import S3_SR\n",
    "from chatterbox.models.s3gen.s3gen import S3Token2Mel\n",
    "# from chatterbox.utils.training_args import CustomTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43851719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from dataclasses import dataclass, field\n",
    "from transformers.training_args import TrainingArguments as HfTrainingArguments\n",
    "\n",
    "# --- Custom Training Arguments ---\n",
    "@dataclass\n",
    "class CustomTrainingArguments(HfTrainingArguments):\n",
    "    early_stopping_patience: Optional[int] = field(\n",
    "        default=None, metadata={\"help\": \"Enable early stopping with specified patience. Default: None (disabled).\"}\n",
    "    )\n",
    "    use_torch_profiler: bool = field(\n",
    "        default=False, metadata={\"help\": \"Enable PyTorch profiler and dump traces to TensorBoard.\"}\n",
    "    )\n",
    "    dataloader_persistent_workers: bool = field(\n",
    "        default=True, metadata={\"help\": \"Use persistent workers for the dataloader.\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c262feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "audio_dir = Path(\"dataset/bengali_audio_files\")\n",
    "manifest_path = Path(\"dataset/bengali_audio_manifest.csv\")\n",
    "\n",
    "# Read manifest and create JSON files\n",
    "with open(manifest_path, encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        audio_filename = row.get(\"audio_filepath\") or row.get(\"audio_path\") or row.get(\"wav\") or row.get(\"filename\")\n",
    "        text = row.get(\"text\") or row.get(\"transcript\") or row.get(\"sentence\")\n",
    "        if not audio_filename or not text:\n",
    "            continue\n",
    "        audio_path = audio_dir / Path(audio_filename).name\n",
    "        json_path = audio_path.with_suffix(\".json\")\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as jf:\n",
    "            json.dump({\"text\": text}, jf, ensure_ascii=False, indent=2)\n",
    "        print(f\"Created {json_path}\")\n",
    "\n",
    "print(\"All JSON files created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1022d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configuration\n",
    "# -------------------\n",
    "# MODEL ARGUMENTS\n",
    "# -------------------\n",
    "# Here we define where to get the base model from.\n",
    "# We'll use the default Chatterbox model from the Hugging Face Hub.\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: str = \"ResembleAI/chatterbox\"\n",
    "    cache_dir: Optional[str] = None\n",
    "    freeze_voice_encoder: bool = True\n",
    "    freeze_s3gen: bool = True\n",
    "\n",
    "model_args = ModelArguments()\n",
    "\n",
    "# -------------------\n",
    "# DATA ARGUMENTS\n",
    "# -------------------\n",
    "# Here we define where your data is and how to process it.\n",
    "# ➡️ **CHANGE `dataset_dir` to the path of your audio/text folder.**\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    dataset_dir: str = \"D:/Code/voice_clone/local_test/custom_train/dataset/bengali_audio_files\" # ⬅️ CHANGE THIS\n",
    "    eval_split_size: float = 0.05 # Use 5% of the data for validation\n",
    "    max_text_len: int = 256\n",
    "    max_speech_len: int = 800\n",
    "    audio_prompt_duration_s: float = 3.0\n",
    "\n",
    "data_args = DataArguments()\n",
    "\n",
    "# -------------------\n",
    "# TRAINING ARGUMENTS\n",
    "# -------------------\n",
    "# Here we define the training parameters (batch size, learning rate, etc.)\n",
    "# ➡️ **CHANGE `output_dir` to where you want to save checkpoints.**\n",
    "training_args = CustomTrainingArguments(\n",
    "    output_dir=\"custom_train/notebooks/checkpoints_bengali\", # ⬅️ CHANGE THIS\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=4,         # Lower this if you run out of GPU memory (e.g., to 2)\n",
    "    gradient_accumulation_steps=4,         # Effective batch size = 4 * 4 = 16\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    fp16=True,                             # Set to False if your GPU doesn't support it\n",
    "    report_to=\"tensorboard\",\n",
    "    dataloader_num_workers=4,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Setup logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32f06c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: SpeechFineTuningDataset Class\n",
    "class SpeechFineTuningDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data_args: DataArguments,\n",
    "                 t3_config: T3Config,\n",
    "                 dataset_source: List[Dict[str, str]],\n",
    "                 chatterbox_model):\n",
    "        self.data_args = data_args\n",
    "        self.chatterbox_t3_config = t3_config\n",
    "        self.dataset_source = dataset_source\n",
    "        self.chatterbox_model = chatterbox_model\n",
    "        self.text_tokenizer = self.chatterbox_model.tokenizer\n",
    "        self.speech_tokenizer = self.chatterbox_model.s3gen.tokenizer\n",
    "        self.voice_encoder = self.chatterbox_model.ve\n",
    "        self.s3_sr = S3_SR\n",
    "        self.enc_cond_audio_len_samples = int(data_args.audio_prompt_duration_s * self.s3_sr)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_source)\n",
    "\n",
    "    def __getitem__(self, idx) -> Optional[Dict[str, Union[torch.Tensor, float]]]:\n",
    "        item = self.dataset_source[idx]\n",
    "        audio_path = item[\"audio\"]\n",
    "        text = item[\"text\"]\n",
    "        \n",
    "        try:\n",
    "            wav_16k, _ = librosa.load(audio_path, sr=self.s3_sr, mono=True)\n",
    "            if wav_16k is None or len(wav_16k) == 0: return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading audio {audio_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        speaker_emb_np = self.voice_encoder.embeds_from_wavs([wav_16k], sample_rate=self.s3_sr)\n",
    "        speaker_emb = torch.from_numpy(speaker_emb_np[0])\n",
    "\n",
    "        normalized_text = punc_norm(text)\n",
    "        \n",
    "        # ==========================================================\n",
    "        # ⬇️ THIS IS WHERE YOU ADD/MODIFY LANGUAGE-SPECIFIC LOGIC ⬇️\n",
    "        # ==========================================================\n",
    "        try:\n",
    "            lang = detect(normalized_text)\n",
    "        except:\n",
    "            lang = \"en\" # Default to English if detection fails\n",
    "\n",
    "        if lang == \"ja\":\n",
    "            pka_converter = pykakasi.kakasi()\n",
    "            pka_converter.setMode(\"J\",\"H\"); pka_converter.setMode(\"K\",\"H\"); pka_converter.setMode(\"H\",\"H\")\n",
    "            conv = pka_converter.getConverter()\n",
    "            normalized_text = conv.do(normalized_text)\n",
    "        elif lang == \"fr\":\n",
    "            normalized_text = \"[fr] \" + normalized_text\n",
    "        elif lang == \"de\":\n",
    "            normalized_text = \"[de] \" + normalized_text\n",
    "        \n",
    "        # --- OUR NEW SPANISH CODE ---\n",
    "        elif lang == \"es\":\n",
    "            # 1. Add the language ID token. The model will learn this means \"speak in Spanish\".\n",
    "            normalized_text = \"[es] \" + normalized_text\n",
    "            \n",
    "            # 2. (Optional but recommended) Add text normalization rules.\n",
    "            # Example: convert \"123\" to \"ciento veintitrés\".\n",
    "            import re\n",
    "            def expand_numbers_es(text):\n",
    "                return re.sub(r'(\\d+)', lambda m: num2words(int(m.group(1)), lang='es'), text)\n",
    "            normalized_text = expand_numbers_es(normalized_text)\n",
    "        # --- END OF NEW LANGUAGE CODE ---\n",
    "\n",
    "        raw_text_tokens = self.text_tokenizer.text_to_tokens(normalized_text).squeeze(0)\n",
    "        text_tokens = F.pad(raw_text_tokens, (1, 0), value=self.chatterbox_t3_config.start_text_token)\n",
    "        text_tokens = F.pad(text_tokens, (0, 1), value=self.chatterbox_t3_config.stop_text_token)\n",
    "        if len(text_tokens) > self.data_args.max_text_len:\n",
    "            text_tokens = text_tokens[:self.data_args.max_text_len-1]\n",
    "            text_tokens = torch.cat([text_tokens, torch.tensor([self.chatterbox_t3_config.stop_text_token])])\n",
    "        text_token_len = torch.tensor(len(text_tokens), dtype=torch.long)\n",
    "\n",
    "        raw_speech_tokens, speech_lens = self.speech_tokenizer.forward([wav_16k])\n",
    "        if raw_speech_tokens is None: return None\n",
    "        raw_speech_tokens = raw_speech_tokens.squeeze(0)[:speech_lens.squeeze(0).item()]\n",
    "        \n",
    "        speech_tokens = F.pad(raw_speech_tokens, (1, 0), value=self.chatterbox_t3_config.start_speech_token)\n",
    "        speech_tokens = F.pad(speech_tokens, (0, 1), value=self.chatterbox_t3_config.stop_speech_token)\n",
    "        if len(speech_tokens) > self.data_args.max_speech_len:\n",
    "            speech_tokens = speech_tokens[:self.data_args.max_speech_len-1]\n",
    "            speech_tokens = torch.cat([speech_tokens, torch.tensor([self.chatterbox_t3_config.stop_speech_token])])\n",
    "        speech_token_len = torch.tensor(len(speech_tokens), dtype=torch.long)\n",
    "\n",
    "        cond_audio = wav_16k[:self.enc_cond_audio_len_samples]\n",
    "        cond_prompt, _ = self.speech_tokenizer.forward([cond_audio], max_len=self.chatterbox_t3_config.speech_cond_prompt_len)\n",
    "        cond_prompt = cond_prompt.squeeze(0) if cond_prompt is not None else torch.zeros(self.chatterbox_t3_config.speech_cond_prompt_len, dtype=torch.long)\n",
    "\n",
    "        if cond_prompt.size(0) != self.chatterbox_t3_config.speech_cond_prompt_len:\n",
    "             cond_prompt = F.pad(cond_prompt, (0, self.chatterbox_t3_config.speech_cond_prompt_len - cond_prompt.size(0)), value=0)\n",
    "\n",
    "        return {\n",
    "            \"text_tokens\": text_tokens.long(), \"text_token_lens\": text_token_len.long(),\n",
    "            \"speech_tokens\": speech_tokens.long(), \"speech_token_lens\": speech_token_len.long(),\n",
    "            \"t3_cond_speaker_emb\": speaker_emb.float(),\n",
    "            \"t3_cond_prompt_speech_tokens\": cond_prompt.long(),\n",
    "            \"t3_cond_emotion_adv\": torch.tensor(0.5, dtype=torch.float),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a29d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Data Collator\n",
    "@dataclass\n",
    "class SpeechDataCollator:\n",
    "    t3_config: T3Config\n",
    "    text_pad_token_id: int\n",
    "    speech_pad_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Optional[Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        valid_features = [f for f in features if f is not None]\n",
    "        if not valid_features: return {}\n",
    "        \n",
    "        text_tokens = [f[\"text_tokens\"] for f in valid_features]\n",
    "        speech_tokens = [f[\"speech_tokens\"] for f in valid_features]\n",
    "        max_text_len = max(len(t) for t in text_tokens)\n",
    "        max_speech_len = max(len(s) for s in speech_tokens)\n",
    "\n",
    "        padded_text = torch.stack([F.pad(t, (0, max_text_len - len(t)), value=self.text_pad_token_id) for t in text_tokens])\n",
    "        padded_speech = torch.stack([F.pad(s, (0, max_speech_len - len(s)), value=self.speech_pad_token_id) for s in speech_tokens])\n",
    "        \n",
    "        IGNORE_ID = -100\n",
    "        labels_text = padded_text[:, 1:].clone()\n",
    "        labels_text[labels_text == self.text_pad_token_id] = IGNORE_ID\n",
    "        \n",
    "        labels_speech = padded_speech[:, 1:].clone()\n",
    "        labels_speech[labels_speech == self.speech_pad_token_id] = IGNORE_ID\n",
    "\n",
    "        return {\n",
    "            \"text_tokens\": padded_text,\n",
    "            \"text_token_lens\": torch.stack([f[\"text_token_lens\"] for f in valid_features]),\n",
    "            \"speech_tokens\": padded_speech,\n",
    "            \"speech_token_lens\": torch.stack([f[\"speech_token_lens\"] for f in valid_features]),\n",
    "            \"t3_cond_speaker_emb\": torch.stack([f[\"t3_cond_speaker_emb\"] for f in valid_features]),\n",
    "            \"t3_cond_prompt_speech_tokens\": torch.stack([f[\"t3_cond_prompt_speech_tokens\"] for f in valid_features]),\n",
    "            \"t3_cond_emotion_adv\": torch.stack([f[\"t3_cond_emotion_adv\"] for f in valid_features]).view(len(valid_features), 1, 1),\n",
    "            \"labels_text\": labels_text,\n",
    "            \"labels_speech\": labels_speech,\n",
    "        }\n",
    "\n",
    "# Cell 6: Model Wrapper for Hugging Face Trainer\n",
    "class T3ForFineTuning(torch.nn.Module):\n",
    "    def __init__(self, t3_model: T3, chatterbox_t3_config: T3Config):\n",
    "        super().__init__()\n",
    "        self.t3 = t3_model\n",
    "        self.chatterbox_t3_config = chatterbox_t3_config\n",
    "        class HFCompatibleConfig(PretrainedConfig): model_type = \"chatterbox_t3_finetune\"\n",
    "        self.config = HFCompatibleConfig()\n",
    "\n",
    "    def forward(self, labels_text=None, labels_speech=None, **kwargs):\n",
    "        current_t3_cond = T3Cond(\n",
    "            speaker_emb=kwargs[\"t3_cond_speaker_emb\"],\n",
    "            cond_prompt_speech_tokens=kwargs[\"t3_cond_prompt_speech_tokens\"],\n",
    "            emotion_adv=kwargs[\"t3_cond_emotion_adv\"]\n",
    "        ).to(device=self.t3.device)\n",
    "        \n",
    "        loss_text, loss_speech, _ = self.t3.loss(\n",
    "            t3_cond=current_t3_cond, text_tokens=kwargs[\"text_tokens\"], text_token_lens=kwargs[\"text_token_lens\"],\n",
    "            speech_tokens=kwargs[\"speech_tokens\"], speech_token_lens=kwargs[\"speech_token_lens\"],\n",
    "            labels_text=labels_text, labels_speech=labels_speech\n",
    "        )\n",
    "        return {\"loss\": loss_text + loss_speech}\n",
    "        \n",
    "# Cell 7: Logging Callback\n",
    "class DetailedLoggingCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and 'loss' in logs:\n",
    "            logger.info(f\"Step {state.global_step}: Loss = {logs['loss']:.4f}, LR = {logs.get('learning_rate', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bc0b42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/18/2025 19:02:29 - INFO - __main__ - Training parameters CustomTrainingArguments(\n",
      "_n_gpu=0,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=True,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "early_stopping_patience=None,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.EPOCH,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=custom_train/notebooks/checkpoints_bengali\\runs\\Sep18_18-50-30_wsthinkpad1,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=20,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=custom_train/notebooks/checkpoints_bengali,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=custom_train/notebooks/checkpoints_bengali,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.EPOCH,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "use_torch_profiler=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=500,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "09/18/2025 19:02:29 - INFO - __main__ - Loading base model from ResembleAI/chatterbox...\n",
      "09/18/2025 19:02:29 - INFO - __main__ - Loading base model from ResembleAI/chatterbox...\n",
      "09/18/2025 19:02:38 - INFO - root - input frame rate=25\n",
      "09/18/2025 19:02:38 - INFO - root - input frame rate=25\n",
      "09/18/2025 19:02:39 - INFO - __main__ - Freezing specified model layers...\n",
      "09/18/2025 19:02:39 - INFO - __main__ - Voice Encoder frozen.\n",
      "09/18/2025 19:02:39 - INFO - __main__ - S3Gen model frozen.\n",
      "09/18/2025 19:02:39 - INFO - __main__ - T3 model set to trainable.\n",
      "09/18/2025 19:02:39 - INFO - __main__ - Found 91 JSON files in D:\\Code\\voice_clone\\local_test\\custom_train\\dataset\\bengali_audio_files.\n",
      "09/18/2025 19:02:39 - INFO - __main__ - Freezing specified model layers...\n",
      "09/18/2025 19:02:39 - INFO - __main__ - Voice Encoder frozen.\n",
      "09/18/2025 19:02:39 - INFO - __main__ - S3Gen model frozen.\n",
      "09/18/2025 19:02:39 - INFO - __main__ - T3 model set to trainable.\n",
      "09/18/2025 19:02:39 - INFO - __main__ - Found 91 JSON files in D:\\Code\\voice_clone\\local_test\\custom_train\\dataset\\bengali_audio_files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded PerthNet (Implicit) at step 250,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset files: 100%|██████████| 91/91 [00:00<00:00, 2261.51it/s]\n",
      "09/18/2025 19:02:39 - INFO - __main__ - Successfully loaded 91 audio-text pairs.\n",
      "09/18/2025 19:02:39 - INFO - __main__ - Split dataset: 86 for training, 5 for evaluation.\n",
      "09/18/2025 19:02:39 - INFO - __main__ - Initializing datasets and data collator...\n",
      "09/18/2025 19:02:39 - INFO - __main__ - Initializing the Hugging Face Trainer...\n",
      "Loading dataset files: 100%|██████████| 91/91 [00:00<00:00, 2261.51it/s]\n",
      "09/18/2025 19:02:39 - INFO - __main__ - Successfully loaded 91 audio-text pairs.\n",
      "09/18/2025 19:02:39 - INFO - __main__ - Split dataset: 86 for training, 5 for evaluation.\n",
      "09/18/2025 19:02:39 - INFO - __main__ - Initializing datasets and data collator...\n",
      "09/18/2025 19:02:39 - INFO - __main__ - Initializing the Hugging Face Trainer...\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Setup Logging and Seed\n",
    "logger.info(\"Training parameters %s\", training_args)\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "# Cell 9: Load Pre-trained Chatterbox Model\n",
    "logger.info(f\"Loading base model from {model_args.model_name_or_path}...\")\n",
    "chatterbox_model = ChatterboxTTS.from_pretrained(\n",
    "    # model_args.model_name_or_path,\n",
    "    # save_dir=download_dir,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "t3_model = chatterbox_model.t3\n",
    "chatterbox_t3_config_instance = t3_model.hp\n",
    "\n",
    "# Cell 10: Freeze Model Layers\n",
    "logger.info(\"Freezing specified model layers...\")\n",
    "if model_args.freeze_voice_encoder:\n",
    "    for param in chatterbox_model.ve.parameters(): param.requires_grad = False\n",
    "    logger.info(\"Voice Encoder frozen.\")\n",
    "if model_args.freeze_s3gen:\n",
    "    for param in chatterbox_model.s3gen.parameters(): param.requires_grad = False\n",
    "    logger.info(\"S3Gen model frozen.\")\n",
    "for param in t3_model.parameters(): param.requires_grad = True\n",
    "logger.info(\"T3 model set to trainable.\")\n",
    "\n",
    "# Cell 11: Load Your Local Dataset Files\n",
    "def load_local_dataset(dataset_dir: str) -> List[Dict[str, str]]:\n",
    "    dataset_path = Path(dataset_dir)\n",
    "    json_files = list(dataset_path.glob(\"**/*.json\"))\n",
    "    files = []\n",
    "    logger.info(f\"Found {len(json_files)} JSON files in {dataset_path}.\")\n",
    "    for json_file in tqdm(json_files, desc=\"Loading dataset files\"):\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        text = data.get(\"text\", \"\").strip()\n",
    "        # Find audio file with the same name but different extension (.wav, .mp3, etc.)\n",
    "        audio_path = next(dataset_path.glob(f\"{json_file.stem}.*\"), None)\n",
    "        if audio_path and audio_path.exists() and text:\n",
    "            files.append({\"audio\": str(audio_path), \"text\": text})\n",
    "    return files\n",
    "\n",
    "all_files = load_local_dataset(data_args.dataset_dir)\n",
    "if not all_files:\n",
    "    raise ValueError(\"No data files found! Check your `dataset_dir` path and file structure.\")\n",
    "logger.info(f\"Successfully loaded {len(all_files)} audio-text pairs.\")\n",
    "\n",
    "np.random.shuffle(all_files)\n",
    "split_idx = int(len(all_files) * (1 - data_args.eval_split_size))\n",
    "train_files, eval_files = all_files[:split_idx], all_files[split_idx:]\n",
    "logger.info(f\"Split dataset: {len(train_files)} for training, {len(eval_files)} for evaluation.\")\n",
    "\n",
    "# Cell 12: Create PyTorch Datasets and Collator\n",
    "logger.info(\"Initializing datasets and data collator...\")\n",
    "train_dataset = SpeechFineTuningDataset(\n",
    "    data_args,\n",
    "    chatterbox_t3_config_instance,\n",
    "    train_files,\n",
    "    chatterbox_model=chatterbox_model,\n",
    ")\n",
    "\n",
    "eval_dataset = SpeechFineTuningDataset(\n",
    "    data_args,\n",
    "    chatterbox_t3_config_instance,\n",
    "    eval_files,\n",
    "    chatterbox_model=chatterbox_model,\n",
    ")\n",
    "\n",
    "data_collator = SpeechDataCollator(\n",
    "    chatterbox_t3_config_instance,\n",
    "    chatterbox_t3_config_instance.stop_text_token,\n",
    "    chatterbox_t3_config_instance.stop_speech_token\n",
    ")\n",
    "\n",
    "# Cell 13: Initialize the Trainer\n",
    "logger.info(\"Initializing the Hugging Face Trainer...\")\n",
    "hf_trainable_model = T3ForFineTuning(t3_model, chatterbox_t3_config_instance)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=hf_trainable_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[DetailedLoggingCallback()],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca544d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/18/2025 19:03:28 - INFO - __main__ - *** Starting T3 model fine-tuning ***\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Cell 14: Run Training\n",
    "logger.info(\"*** Starting T3 model fine-tuning ***\")\n",
    "train_result = trainer.train()\n",
    "logger.info(\"*** Training finished ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec89ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Save Final Model Artifacts\n",
    "logger.info(\"Saving the final fine-tuned model...\")\n",
    "\n",
    "# 1. Save the fine-tuned T3 weights\n",
    "final_output_dir = Path(training_args.output_dir) / \"final_model\"\n",
    "final_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_t3_path = final_output_dir / \"t3_cfg.safetensors\"\n",
    "\n",
    "t3_to_save = trainer.model.t3\n",
    "from safetensors.torch import save_file\n",
    "save_file(t3_to_save.state_dict(), output_t3_path)\n",
    "logger.info(f\"T3 weights saved to {output_t3_path}\")\n",
    "\n",
    "# 2. Copy the other essential model files\n",
    "import shutil\n",
    "files_to_copy = [\"ve.safetensors\", \"s3gen.safetensors\", \"tokenizer.json\", \"conds.pt\"]\n",
    "for f_name in files_to_copy:\n",
    "    src_path = download_dir / f_name\n",
    "    if src_path.exists():\n",
    "        shutil.copy2(src_path, final_output_dir / f_name)\n",
    "        logger.info(f\"Copied {f_name} to final model directory.\")\n",
    "\n",
    "logger.info(f\"✅ Complete fine-tuned model saved in: {final_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4289be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatterbox.tts import ChatterboxTTS\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Path to your fine-tuned model directory\n",
    "MODEL_PATH = Path(\"/path/to/your/checkpoints/final_model\")\n",
    "\n",
    "# 2. Path to a short audio clip of the target speaker's voice\n",
    "# This is used to create the speaker embedding. It can be any clip from your training data.\n",
    "SPEAKER_WAV_PATH = \"/path/to/your/spanish_dataset/some_audio_file.wav\"\n",
    "\n",
    "# 3. Load your fine-tuned model\n",
    "print(\"Loading the fine-tuned model...\")\n",
    "model = ChatterboxTTS.from_local(ckpt_dir=str(MODEL_PATH), device=\"cuda\") # or \"cpu\"\n",
    "\n",
    "# 4. Define the text you want to generate\n",
    "# IMPORTANT: Use the language token you trained on!\n",
    "text_to_generate_bengali = \"[bn] আমি বাংলায় কথা বলি।\"\n",
    "text_to_generate_english = \"Hello, this is a test in English.\" # This should also work!\n",
    "\n",
    "# 5. Generate the audio\n",
    "print(f\"Generating audio for: {text_to_generate_bengali}\")\n",
    "wav_bengali = model.generate(\n",
    "    text=text_to_generate_bengali,\n",
    "    speaker_wav=SPEAKER_WAV_PATH\n",
    ")\n",
    "\n",
    "print(f\"Generating audio for: {text_to_generate_english}\")\n",
    "wav_english = model.generate(\n",
    "    text=text_to_generate_english,\n",
    "    speaker_wav=SPEAKER_WAV_PATH\n",
    ")\n",
    "\n",
    "# 6. Save the audio to a file\n",
    "sf.write(\"output_bengali.wav\", wav_bengali, samplerate=24000)\n",
    "sf.write(\"output_english.wav\", wav_english, samplerate=24000)\n",
    "\n",
    "print(\"✅ Audio files saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_test (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
