{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d53ba5a4",
   "metadata": {},
   "source": [
    "# Quick Bengali T3+S3 joint training on 10 samples\n",
    "This notebook runs a minimal training loop over 10 samples from eucalyptus/shrutilipi_bengali, computing both T3 (text→speech tokens) and S3 (tokens→mel) losses and plotting them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317eb0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb2e8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Setup Environment and GPU\n",
    "import sys, subprocess, importlib, os\n",
    "\n",
    "# pkgs = [\n",
    "#     (\"datasets[audio]\", \"datasets\"),\n",
    "#     (\"librosa\", \"librosa\"),\n",
    "#     (\"soundfile\", \"soundfile\"),\n",
    "#     (\"matplotlib\", \"matplotlib\"),\n",
    "# ]\n",
    "# for spec, mod in pkgs:\n",
    "#     try:\n",
    "#         importlib.import_module(mod)\n",
    "#     except ImportError:\n",
    "#         print(f\"Installing {spec}...\")\n",
    "#         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", spec])\n",
    "\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39384cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Configure Run (paths, hyperparameters, N_SAMPLES=10)\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(r\"d:\\Code\\voice_clone\\local_test\").resolve()\n",
    "CHECKPOINT_DIR = ROOT / \"custom_train\" / \"checkpoints_bengali_notebook\"\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TOKENIZER_PATH = str(ROOT / \"tokenizer_bn_tts.json\")\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "ACCUM_STEPS = 1\n",
    "LEARNING_RATE = 3e-4\n",
    "N_SAMPLES = 10\n",
    "S3GEN_SR = 24000\n",
    "S3_SR = 16000\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a15e395f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\voice_clone\\local_test\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3_SR: 16000 S3GEN_SR: 24000\n"
     ]
    }
   ],
   "source": [
    "# Add Project To Python Path and Imports\n",
    "import sys, os\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from custom_train.custom_llama_configs import T3Config\n",
    "from src.chatterbox.models.t3.t3 import T3\n",
    "from custom_train.custom_modules.custom_cond_enc import T3Cond\n",
    "from bengalitokenization import BengaliTokenizer\n",
    "from src.chatterbox.models.s3tokenizer.s3tokenizer import S3Tokenizer, S3_SR as S3_SR_CONST\n",
    "from src.chatterbox.models.s3gen.s3gen import S3Token2Mel\n",
    "from src.chatterbox.models.s3gen.utils.mel import mel_spectrogram\n",
    "from src.chatterbox.models.s3gen.utils.mask import make_pad_mask\n",
    "from src.chatterbox.models.voice_encoder.voice_encoder import VoiceEncoder\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# keep constants consistent\n",
    "S3_SR = S3_SR_CONST\n",
    "print(\"S3_SR:\", S3_SR, \"S3GEN_SR:\", S3GEN_SR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f9e953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torchcodec==0.2 (from versions: 0.0.0.dev0, 0.7.0)\n",
      "ERROR: No matching distribution found for torchcodec==0.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchcodec==0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99caa321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "audio_path = r\"D:\\Code\\voice_clone\\local_test\\dataset\\bengali_audio_files\\audio_00002.wav\"\n",
    "print(os.path.exists(audio_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acb5c16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (191360,)\n",
      "Sample rate: 16000\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "audio_path = ROOT / \"dataset\" / \"bengali_audio_files\" / \"audio_00002.wav\"\n",
    "data, samplerate = sf.read(audio_path)\n",
    "\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"Sample rate:\", samplerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f0ca303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['audio', 'transcriptions', 'accent'] len: 10\n",
      "audio             {'array': [0.0014218403828098098, 0.0077988284...\n",
      "transcriptions    আসামে জাতীয় নাগরিক পঞ্জীর চূড়ান্ত খসড়া আগাম...\n",
      "accent                                                     standard\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load local CSV manifest and decode audio manually with soundfile\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "manifest_path = ROOT / \"dataset\" / \"bengali_audio_manifest.csv\"\n",
    "df = pd.read_csv(manifest_path)\n",
    "\n",
    "# Align column names with our Dataset impl: expects 'audio' and 'transcriptions'\n",
    "df = df.rename(columns={\"audio_path\": \"audio\", \"text\": \"transcriptions\"})\n",
    "\n",
    "# Keep it small for quick run\n",
    "df = df.iloc[:min(N_SAMPLES, len(df))].copy()\n",
    "\n",
    "AUDIO_DIR = ROOT / \"dataset\" / \"bengali_audio_files\"\n",
    "\n",
    "# Decode audio files with soundfile and store as numpy arrays\n",
    "def load_audio(path):\n",
    "    resolved = ROOT / \"dataset\" / Path(path)\n",
    "    try:\n",
    "        wav, sr = sf.read(resolved)\n",
    "        if wav.ndim > 1:\n",
    "            wav = np.mean(wav, axis=1)\n",
    "        if sr != S3GEN_SR:\n",
    "            wav = librosa.resample(wav, orig_sr=sr, target_sr=S3GEN_SR)\n",
    "            sr = S3GEN_SR\n",
    "        return {\"array\": wav, \"sampling_rate\": sr}\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {resolved}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Only load audio if not already present\n",
    "if \"audio\" in df.columns:\n",
    "    df[\"audio\"] = df[\"audio\"].apply(load_audio)\n",
    "\n",
    "# Optional: add a default accent label\n",
    "df[\"accent\"] = \"standard\"\n",
    "\n",
    "print(\"Columns:\", df.columns.tolist(), \"len:\", len(df))\n",
    "print(df.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d95626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: Patch/Validate Fields (language_id, accent_id)\n",
    "# # This dataset has only audio and transcriptions. We can add a default accent.\n",
    "\n",
    "# def add_default_accent(batch):\n",
    "#     batch['accent'] = 'standard'\n",
    "#     return batch\n",
    "\n",
    "# ds = ds.map(add_default_accent)\n",
    "# print(\"Columns:\", ds.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "575bb2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config hidden size: 1024\n"
     ]
    }
   ],
   "source": [
    "# Initialize Tokenizers and Voice Encoder\n",
    "hp = T3Config.bengali_accent(TOKENIZER_PATH)\n",
    "hp.emotion_adv = False\n",
    "text_tok = BengaliTokenizer(TOKENIZER_PATH)\n",
    "s3_tok = S3Tokenizer().to(device)\n",
    "ve = VoiceEncoder().to(device)\n",
    "ve.eval()\n",
    "for p in ve.parameters():\n",
    "    p.requires_grad_(False)\n",
    "print(\"Config hidden size:\", hp.n_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f21f7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min value is  tensor(-1.0437)\n",
      "max value is  tensor(1.0356)\n",
      "Batch keys: ['text_tokens', 'text_token_lens', 'speech_tokens_t3', 'speech_token_lens_t3', 'speech_tokens_raw', 'speech_token_lens_raw', 'mel24', 'mel_len24', 'speaker_emb', 'language_id', 'accent_id']\n",
      "min value is  tensor(-1.0437)\n",
      "max value is  tensor(1.0356)\n",
      "min value is  tensor(-1.0437)\n",
      "max value is  tensor(1.0356)\n",
      "text_tokens: torch.Size([114]) mel24: torch.Size([309, 80])\n",
      "min value is  tensor(-1.0289)\n",
      "max value is  tensor(1.0623)\n",
      "min value is  tensor(-1.0868)\n",
      "max value is  tensor(1.0811)\n",
      "Batch keys: ['text_tokens', 'text_token_lens', 'speech_tokens_t3', 'speech_token_lens_t3', 'speech_tokens_raw', 'speech_token_lens_raw', 'mel24', 'mel_len24', 'speaker_emb', 'language_id', 'accent_id']\n",
      "text_tokens: torch.Size([2, 355]) mel24: torch.Size([2, 1097, 80])\n"
     ]
    }
   ],
   "source": [
    "# Build PyTorch Dataset and DataLoader (reuse your code)\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import librosa\n",
    "\n",
    "# Reuse helpers from the training script\n",
    "from custom_train.train_bengali_t3 import audio_to_tensor_pair, BengaliTTSDataset, collate_fn\n",
    "\n",
    "# Ensure df does not contain rows with failed audio loading\n",
    "if \"audio_array\" in df.columns:\n",
    "    df = df[df[\"audio_array\"].apply(lambda x: x is not None)]\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "train_ds = BengaliTTSDataset(df, hp=hp, text_tok=text_tok, s3_tok=s3_tok, ve=ve)\n",
    "loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "print(\"Batch keys:\", list(train_ds[0].keys()))\n",
    "print(\"text_tokens:\", train_ds[0]['text_tokens'].shape, \"mel24:\", train_ds[0]['mel24'].shape)\n",
    "\n",
    "batch = next(iter(loader))\n",
    "print(\"Batch keys:\", list(batch.keys()))\n",
    "print(\"text_tokens:\", batch['text_tokens'].shape, \"mel24:\", batch['mel24'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a74d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Models, Optimizer, and AMP Scaler\n",
    "t3_model = T3(hp=hp).to(device)\n",
    "s3_model = S3Token2Mel().to(device)\n",
    "optimizer = torch.optim.AdamW(list(t3_model.parameters()) + list(s3_model.parameters()), lr=LEARNING_RATE)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "t3_model.train(); s3_model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "def7b4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min value is  tensor(-1.0348)\n",
      "max value is  tensor(1.0371)\n",
      "min value is  tensor(-1.0868)\n",
      "max value is  tensor(1.0811)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shubham Halder\\AppData\\Local\\Temp\\ipykernel_21212\\3113389641.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "Epoch 1/1:   0%|          | 0/5 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [2, 16000], got [2, 355]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     21\u001b[39m cond = T3Cond(\n\u001b[32m     22\u001b[39m     speaker_emb=spk,\n\u001b[32m     23\u001b[39m     language_id=torch.full((text_tokens.size(\u001b[32m0\u001b[39m),), \u001b[32m1\u001b[39m, dtype=torch.long, device=device),\n\u001b[32m     24\u001b[39m     accent_id=torch.zeros((text_tokens.size(\u001b[32m0\u001b[39m),), dtype=torch.long, device=device),\n\u001b[32m     25\u001b[39m     emotion_adv=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     26\u001b[39m )\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m# T3 losses\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     loss_text, loss_speech = \u001b[43mt3_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt3_cond\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_token_lens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_token_lens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspeech_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeech_tokens_t3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspeech_token_lens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeech_token_lens_t3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# S3 losses\u001b[39;00m\n\u001b[32m     39\u001b[39m     spk_norm = F.normalize(spk, dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Code\\voice_clone\\local_test\\src\\chatterbox\\models\\t3\\t3.py:201\u001b[39m, in \u001b[36mT3.loss\u001b[39m\u001b[34m(self, t3_cond, text_tokens, text_token_lens, speech_tokens, speech_token_lens)\u001b[39m\n\u001b[32m    199\u001b[39m masked_text = text_tokens.masked_fill(mask_text, IGNORE_ID)\n\u001b[32m    200\u001b[39m masked_speech = speech_tokens.masked_fill(mask_speech, IGNORE_ID)\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m loss_text = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasked_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIGNORE_ID\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m loss_speech = F.cross_entropy(out.speech_logits, masked_speech, ignore_index=IGNORE_ID)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss_text, loss_speech\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code\\voice_clone\\local_test\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected target size [2, 16000], got [2, 355]"
     ]
    }
   ],
   "source": [
    "# Short Training Loop (collect losses)\n",
    "from tqdm import tqdm\n",
    "\n",
    "L_txt, L_sp, L_s3, L_total = [], [], [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    step = 0\n",
    "    optimizer.zero_grad()\n",
    "    for batch in pbar:\n",
    "        text_tokens = batch[\"text_tokens\"].to(device)\n",
    "        text_token_lens = batch[\"text_token_lens\"].to(device)\n",
    "        speech_tokens_t3 = batch[\"speech_tokens_t3\"].to(device)\n",
    "        speech_token_lens_t3 = batch[\"speech_token_lens_t3\"].to(device)\n",
    "        speech_tokens_raw = batch[\"speech_tokens_raw\"].to(device)\n",
    "        speech_token_lens_raw = batch[\"speech_token_lens_raw\"].to(device)\n",
    "        mel24 = batch[\"mel24\"].to(device)\n",
    "        mel_len24 = batch[\"mel_len24\"].to(device)\n",
    "        spk = batch[\"speaker_emb\"].to(device)\n",
    "\n",
    "        cond = T3Cond(\n",
    "            speaker_emb=spk,\n",
    "            language_id=torch.full((text_tokens.size(0),), 1, dtype=torch.long, device=device),\n",
    "            accent_id=torch.zeros((text_tokens.size(0),), dtype=torch.long, device=device),\n",
    "            emotion_adv=None,\n",
    "        )\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            # T3 losses\n",
    "            loss_text, loss_speech = t3_model.loss(\n",
    "                t3_cond=cond,\n",
    "                text_tokens=text_tokens,\n",
    "                text_token_lens=text_token_lens,\n",
    "                speech_tokens=speech_tokens_t3,\n",
    "                speech_token_lens=speech_token_lens_t3,\n",
    "            )\n",
    "\n",
    "            # S3 losses\n",
    "            spk_norm = F.normalize(spk, dim=1)\n",
    "            spk_proj = s3_model.flow.spk_embed_affine_layer(spk_norm)\n",
    "            mask_tok = (~make_pad_mask(speech_token_lens_raw)).float().unsqueeze(-1).to(device)\n",
    "            tok_emb = s3_model.flow.input_embedding(torch.clamp(speech_tokens_raw, min=0)) * mask_tok\n",
    "            h, _ = s3_model.flow.encoder(tok_emb, speech_token_lens_raw)\n",
    "            h = s3_model.flow.encoder_proj(h)             # (B, Tm, 80)\n",
    "\n",
    "            Tm = int(min(h.shape[1], int(mel_len24.min().item())))\n",
    "            h = h[:, :Tm, :].transpose(1, 2).contiguous()\n",
    "            feat = mel24[:, :Tm, :].transpose(1, 2).contiguous()\n",
    "            mask_mel = (~make_pad_mask(torch.full_like(mel_len24, Tm))).unsqueeze(1).float().to(device)\n",
    "            cond_mel = torch.zeros_like(feat)\n",
    "            loss_s3, _ = s3_model.flow.decoder.compute_loss(x1=feat, mask=mask_mel, mu=h, spks=spk_proj, cond=cond_mel)\n",
    "\n",
    "            total_loss = loss_text + loss_speech + loss_s3\n",
    "\n",
    "        scaler.scale(total_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        L_txt.append(loss_text.item()); L_sp.append(loss_speech.item()); L_s3.append(loss_s3.item()); L_total.append(total_loss.item())\n",
    "        pbar.set_postfix({\"L_txt\": f\"{L_txt[-1]:.3f}\", \"L_sp\": f\"{L_sp[-1]:.3f}\", \"L_s3\": f\"{L_s3[-1]:.3f}\", \"L\": f\"{L_total[-1]:.3f}\"})\n",
    "        step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe972d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training Loss Curves\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(L_txt, label='L_t3_txt')\n",
    "plt.plot(L_sp, label='L_t3_sp')\n",
    "plt.plot(L_s3, label='L_s3')\n",
    "plt.plot(L_total, label='L_total', linewidth=2)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training losses (10 samples)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09da2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Checkpoint and List Files\n",
    "ckpt_path = CHECKPOINT_DIR / \"t3_s3_bn_quickrun.pt\"\n",
    "state = {\n",
    "    \"t3_model\": t3_model.state_dict(),\n",
    "    \"s3_model\": s3_model.state_dict(),\n",
    "    \"hp\": hp.__dict__,\n",
    "}\n",
    "import torch\n",
    "torch.save(state, str(ckpt_path))\n",
    "print(\"Saved:\", ckpt_path)\n",
    "print(\"Dir:\", list(CHECKPOINT_DIR.iterdir()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_test (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
